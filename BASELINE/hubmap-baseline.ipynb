{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# HuBMAP - BASELINE\n"]},{"cell_type":"markdown","metadata":{},"source":["![](https://storage.googleapis.com/kaggle-competitions/kaggle/22990/logos/header.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"1\"></a>\n","<h2 style='background:#EAA6D1; border:0; color:white'><center>BASELINE CODE<center><h2>"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-28T13:47:46.108670Z","iopub.status.busy":"2023-06-28T13:47:46.108148Z","iopub.status.idle":"2023-06-28T13:47:47.228419Z","shell.execute_reply":"2023-06-28T13:47:47.227513Z","shell.execute_reply.started":"2023-06-28T13:47:46.108612Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import cv2\n","import tifffile\n","import json\n","from PIL import Image\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torchvision\n","from torch.utils.data import Dataset\n","from torch import nn\n","from torch.nn import functional as F\n","from torchvision import models\n","import random\n","from torch.utils.data import DataLoader\n","from torch import optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from tqdm.auto import tqdm\n","\n","import warnings\n","warnings.filterwarnings(action='ignore') \n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","import cv2\n","from torchvision.transforms import ToPILImage\n","from torchvision.transforms import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","import collections\n","import time"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Configuration"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['tile_meta.csv', 'maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth', 'wsi_meta.csv', 'test', 'train', 'polygons.jsonl', 'sample_submission.csv']\n"]}],"source":["# Device & Path\n","DEVICE = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","BASE_PATH = \"../DATA\"\n","\n","TEST_PATH = os.path.join(BASE_PATH, \"test\")\n","\n","print(os.listdir(BASE_PATH))\n","\n","TRAIN_CSV = os.path.join(BASE_PATH, \"train\", \"df_train.csv\")\n","\n","TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n","TEST_PATH = os.path.join(BASE_PATH, \"test\")\n","\n","WIDTH = 512\n","HEIGHT = 512\n","\n","resize_factor = False # 0.5\n","\n","BATCH_SIZE = 2\n","NUM_EPOCHS = 1\n","\n","# Normalize to resnet mean and std if True.\n","NORMALIZE = False\n","RESNET_MEAN = (0.485, 0.456, 0.406)\n","RESNET_STD = (0.229, 0.224, 0.225)\n","\n","# No changes tried with the optimizer yet.\n","MOMENTUM = 0.9\n","LEARNING_RATE = 0.001\n","WEIGHT_DECAY = 0.0005\n","\n","# cell type specific thresholds\n","cell_type_dict = {'blood_vessel': 1, 'glomerulus': 2, 'unsure': 3}\n","mask_threshold_dict = {1: 0.40, 2: 0.80, 3:  0.80}\n","min_score_dict = {1: 0.40, 2: 0.80, 3: 0.80}\n","\n","# Use a StepLR scheduler if True. \n","USE_SCHEDULER = False\n","TEST_SIZE=0.20\n","BOX_DETECTIONS_PER_IMG = 500"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.backends.mps.is_available()"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter Setting"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# CFG = {\n","#     'IMG_SIZE':224,\n","#     'EPOCHS':50,\n","#     'LEARNING_RATE':3e-4,\n","#     'BATCH_SIZE':64,\n","#     'SEED':41\n","# }"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Utilites"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# ref: https://www.kaggle.com/inversion/run-length-decoding-quick-start\n","def rle_decode(mask_rle, shape, color=1):\n","    '''\n","    mask_rle: run-length as string formated (start length)\n","    shape: (height, width, channels) of array to return\n","    color: color for the mask\n","    Returns numpy array (mask)\n","\n","    '''\n","    s = mask_rle.split()\n","\n","    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n","    lengths = list(map(int, s[1::2]))\n","    ends = [x + y for x, y in zip(starts, lengths)]\n","    if len(shape)==3:\n","        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n","    else:\n","        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n","    for start, end in zip(starts, ends):\n","        img[start : end] = color\n","\n","    return img.reshape(shape)\n","\n","\n","def rle_encoding(x):\n","    dots = np.where(x.flatten() == 1)[0]\n","    run_lengths = []\n","    prev = -2\n","    for b in dots:\n","        if (b>prev+1): run_lengths.extend((b + 1, 0))\n","        run_lengths[-1] += 1\n","        prev = b\n","    return ' '.join(map(str, run_lengths))\n","\n","\n","def remove_overlapping_pixels(mask, other_masks):\n","    for other_mask in other_masks:\n","        if np.sum(np.logical_and(mask, other_mask)) > 0:\n","            mask[np.logical_and(mask, other_mask)] = 0\n","    return mask\n","\n","def combine_masks(masks, mask_threshold):\n","    \"\"\"\n","    combine masks into one image\n","    \"\"\"\n","    maskimg = np.zeros((HEIGHT, WIDTH))\n","    # print(len(masks.shape), masks.shape)\n","    for m, mask in enumerate(masks,1):\n","        maskimg[mask>mask_threshold] = m\n","    return maskimg\n","\n","\n","def get_filtered_masks(pred):\n","    \"\"\"\n","    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n","    \"\"\"\n","    use_masks = []\n","    use_labels = []\n","    for i, mask in enumerate(pred[\"masks\"]):\n","        # Filter-out low-scoring results. Not tried yet.\n","        scr = pred[\"scores\"][i].cpu().item()\n","        label = pred[\"labels\"][i].cpu().item()\n","        if scr > min_score_dict[label]:\n","            mask = mask.cpu().numpy().squeeze()\n","            # Keep only highly likely pixels\n","            binary_mask = mask > mask_threshold_dict[label]\n","            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n","            use_masks.append(binary_mask)\n","            use_labels.append(label)\n","\n","    return use_masks,use_labels\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# maP IoU"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def compute_iou(labels, y_pred, verbose=0):\n","    \"\"\"\n","    Computes the IoU for instance labels and predictions.\n","\n","    Args:\n","        labels (np array): Labels.\n","        y_pred (np array): predictions\n","\n","    Returns:\n","        np array: IoU matrix, of size true_objects x pred_objects.\n","    \"\"\"\n","\n","    true_objects = len(np.unique(labels))\n","    pred_objects = len(np.unique(y_pred))\n","\n","    if verbose:\n","        print(\"Number of true objects: {}\".format(true_objects))\n","        print(\"Number of predicted objects: {}\".format(pred_objects))\n","\n","    # Compute intersection between all objects\n","    intersection = np.histogram2d(\n","        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n","    )[0]\n","\n","    # Compute areas (needed for finding the union between all objects)\n","    area_true = np.histogram(labels, bins=true_objects)[0]\n","    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n","    area_true = np.expand_dims(area_true, -1)\n","    area_pred = np.expand_dims(area_pred, 0)\n","\n","    # Compute union\n","    union = area_true + area_pred - intersection\n","    intersection = intersection[1:, 1:] # exclude background\n","    union = union[1:, 1:]\n","    union[union == 0] = 1e-9\n","    iou = intersection / union\n","    \n","    return iou  \n","\n","def precision_at(threshold, iou):\n","    \"\"\"\n","    Computes the precision at a given threshold.\n","\n","    Args:\n","        threshold (float): Threshold.\n","        iou (np array): IoU matrix.\n","\n","    Returns:\n","        int: Number of true positives,\n","        int: Number of false positives,\n","        int: Number of false negatives.\n","    \"\"\"\n","    matches = iou > threshold\n","    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n","    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n","    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n","    tp, fp, fn = (\n","        np.sum(true_positives),\n","        np.sum(false_positives),\n","        np.sum(false_negatives),\n","    )\n","    return tp, fp, fn\n","\n","def iou_map(truths, preds, verbose=0):\n","    \"\"\"\n","    Computes the metric for the competition.\n","    Masks contain the segmented pixels where each object has one value associated,\n","    and 0 is the background.\n","\n","    Args:\n","        truths (list of masks): Ground truths.\n","        preds (list of masks): Predictions.\n","        verbose (int, optional): Whether to print infos. Defaults to 0.\n","\n","    Returns:\n","        float: mAP.\n","    \"\"\"\n","    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n","\n","    if verbose:\n","        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n","\n","    prec = []\n","    for t in np.arange(0.6, 6.5, 0.05):\n","        tps, fps, fns = 0, 0, 0\n","        for iou in ious:\n","            tp, fp, fn = precision_at(t, iou)\n","            tps += tp\n","            fps += fp\n","            fns += fn\n","\n","        p = tps / (tps + fps + fns)\n","        prec.append(p)\n","\n","        if verbose:\n","            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n","\n","    if verbose:\n","        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n","\n","    return np.mean(prec)\n","\n","\n","def get_score(ds, mdl):\n","    \"\"\"\n","    Get average IOU mAP score for a dataset\n","    \"\"\"\n","    mdl.eval()\n","    iouscore = 0\n","    for i in tqdm(range(len(ds))):\n","        img, targets = ds[i]\n","        with torch.no_grad():\n","            result = mdl([img.to(DEVICE)])[0]\n","            \n","        masks = combine_masks(targets['masks'], 0.5)\n","        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n","\n","        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n","        masks_p,labels_p=get_filtered_masks(result)\n","        pred_masks = combine_masks(masks_p, mask_threshold)\n","        iouscore += iou_map([masks],[pred_masks])\n","    return iouscore / len(ds)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Transformations\n","Just Horizontal and Vertical Flip for now.\n","\n","Normalization to Resnet's mean and std can be performed using the parameter `NORMALIZE` in the top cell.\n","\n","The first 3 transformations come from [this](https://www.kaggle.com/abhishek/maskrcnn-utils) utils package by Abishek, `VerticalFlip` is my adaption of HorizontalFlip, and `Normalize` is of my own."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# These are slight redefinitions of torch.transformation classes\n","# The difference is that they handle the target and the mask\n","# Copied from Abishek, added new ones\n","class Compose:\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, image, target):\n","        for t in self.transforms:\n","            image, target = t(image, target)\n","        return image, target\n","\n","class VerticalFlip:\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, image, target):\n","        if random.random() < self.prob:\n","            height, width = image.shape[-2:]\n","            image = image.flip(-2)\n","            bbox = target[\"boxes\"]\n","            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n","            target[\"boxes\"] = bbox\n","            target[\"masks\"] = target[\"masks\"].flip(-2)\n","        return image, target\n","\n","class HorizontalFlip:\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, image, target):\n","        if random.random() < self.prob:\n","            height, width = image.shape[-2:]\n","            image = image.flip(-1)\n","            bbox = target[\"boxes\"]\n","            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n","            target[\"boxes\"] = bbox\n","            target[\"masks\"] = target[\"masks\"].flip(-1)\n","        return image, target\n","\n","class Normalize:\n","    def __call__(self, image, target):\n","        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n","        return image, target\n","\n","class ToTensor:\n","    def __call__(self, image, target):\n","        image = F.to_tensor(image)\n","        return image, target\n","    \n","\n","def get_transform(train):\n","    transforms = [ToTensor()]\n","    if NORMALIZE:\n","        transforms.append(Normalize())\n","    \n","    # Data augmentation for train\n","    if train: \n","        transforms.append(HorizontalFlip(0.5))\n","        transforms.append(VerticalFlip(0.5))\n","\n","    return Compose(transforms)\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset & Dataloader"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","cell_type_dict = {'blood_vessel': 1, 'glomerulus': 2, 'unsure': 3}\n","class HuBMAPDataset(Dataset):\n","    def __init__(self, image_dir, df, transforms=None, resize=False):\n","        self.transforms = transforms\n","        self.image_dir = image_dir\n","        self.df = df\n","        \n","        self.should_resize = resize is not False\n","        if self.should_resize:\n","            self.height = int(HEIGHT * resize)\n","            self.width = int(WIDTH * resize)\n","            print(\"image size used:\", self.height, self.width)\n","        else:\n","            self.height = HEIGHT\n","            self.width = WIDTH\n","        \n","        self.image_info = collections.defaultdict(dict)\n","        temp_df = self.df.groupby([\"image_id\"])[['annotations','category_id']].agg(lambda x: list(x)).reset_index()\n","        for index, row in temp_df.iterrows():\n","            self.image_info[index] = {\n","                    'image_id': row['image_id'],\n","                    'image_path': os.path.join(self.image_dir, row['image_id'] + '.tif'),\n","                    'annotations': list(row[\"annotations\"]),\n","                    'category_name': list(row[\"category_id\"])\n","                    }\n","            \n","    def get_box(self, a_mask):\n","        ''' Get the bounding box of a given mask '''\n","        pos = np.where(a_mask)\n","        xmin = np.min(pos[1])\n","        xmax = np.max(pos[1])\n","        ymin = np.min(pos[0])\n","        ymax = np.max(pos[0])\n","        return [xmin, ymin, xmax, ymax]\n","\n","    def __getitem__(self, idx):\n","        ''' Get the image and the target'''\n","        \n","        img_path = self.image_info[idx][\"image_path\"]\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        info = self.image_info[idx]\n","\n","        n_objects = len(info['annotations'])\n","        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n","        boxes = []\n","        labels = []\n","        for i, annotation in enumerate(info['annotations']):\n","            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))        \n","            a_mask = np.array(a_mask) > 0\n","            masks[i, :, :] = a_mask\n","            boxes.append(self.get_box(a_mask))\n","        labels = info[\"category_name\"]        \n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n","        target = {\n","            'boxes': boxes,\n","            'labels': labels,\n","            'masks': masks,\n","            'image_id': image_id,\n","            'area': area,\n","            'iscrowd': iscrowd\n","        }\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target#,img_path\n","\n","    def __len__(self):\n","        return len(self.image_info)"]},{"cell_type":"markdown","metadata":{},"source":["## Fixed RandomSeed"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(42) # Seed 고정"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#json file load\n","data = []\n","\n","with open(os.path.join(BASE_PATH, 'polygons.jsonl'), 'r') as file:\n","    for line in file:\n","        data.append(json.loads(line))\n","\n","# 한 이미지에 대한 정보만 추출\n","image_data = data[0]\n","\n","image_id = image_data['id']\n","annotations = image_data['annotations']\n","\n","# 이미지 읽어서 크기 얻기\n","image = Image.open(os.path.join(TRAIN_PATH, f'{image_id}.tif')) \n","width, height = image.size\n","\n","# 이미지와 같은 크기의 빈 마스크 생성\n","mask = np.zeros((height, width), dtype=np.uint8)\n","\n","# 각 객체에 대한 정보 추출\n","for annotation in annotations:\n","    mask_type = annotation['type']\n","    coordinates = annotation['coordinates']\n","\n","    # 각 폴리곤 좌표를 numpy 배열로 변환\n","    for polygon in coordinates:\n","        contour = np.array(polygon, dtype=np.int32)\n","\n","        # OpenCV의 fillPoly 함수를 사용하여 마스크에 폴리곤 그리기\n","        cv2.fillPoly(mask, [contour], color=(255))\n","\n","plt.imshow(mask, cmap='gray')\n","plt.show()\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["df_ = pd.read_csv(TRAIN_CSV)\n","df_.head(3)\n","df_images = df_.groupby([\"image_id\", \"category_name\"]).agg({'annotations': 'count'}).sort_values(\"annotations\", ascending=False).reset_index()\n","# Use the quantiles of amoount of annotations to stratify\n","df_images_train, df_images_val = train_test_split(df_images, stratify=df_images['category_name'], \n","                                                  test_size=TEST_SIZE,\n","                                                  random_state=42)\n","df_train = df_[df_['image_id'].isin(df_images_train['image_id'])]\n","df_val = df_[df_['image_id'].isin(df_images_val['image_id'])]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>image_id</th>\n","      <th>category_id</th>\n","      <th>category_name</th>\n","      <th>annotations</th>\n","      <th>bbox</th>\n","      <th>area</th>\n","      <th>iscrowd</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0006ff2aa7cd</td>\n","      <td>2</td>\n","      <td>glomerulus</td>\n","      <td>2 253 513 254 1025 254 1537 254 2049 254 2561 ...</td>\n","      <td>(0, 0, 271, 250)</td>\n","      <td>56426</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0006ff2aa7cd</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>37657 3 38167 7 38678 8 39190 10 39701 11 4021...</td>\n","      <td>(272, 73, 20, 37)</td>\n","      <td>570</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0006ff2aa7cd</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>121934 3 122444 7 122955 9 123466 10 123978 11...</td>\n","      <td>(72, 238, 37, 55)</td>\n","      <td>1027</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0006ff2aa7cd</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>199677 4 200187 6 200697 8 201208 9 201719 10 ...</td>\n","      <td>(489, 389, 23, 54)</td>\n","      <td>996</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0006ff2aa7cd</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>222553 10 223053 5 223063 13 223564 25 224073 ...</td>\n","      <td>(326, 434, 51, 44)</td>\n","      <td>1055</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17513</th>\n","      <td>17513</td>\n","      <td>ffd3d193c71e</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>146108 3 146618 6 147128 8 147639 12 148149 15...</td>\n","      <td>(173, 285, 23, 24)</td>\n","      <td>396</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17514</th>\n","      <td>17514</td>\n","      <td>ffd3d193c71e</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>16940 3 17451 5 17962 7 18473 8 18984 9 19495 ...</td>\n","      <td>(28, 33, 21, 60)</td>\n","      <td>736</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17515</th>\n","      <td>17515</td>\n","      <td>ffd3d193c71e</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>235285 8 235796 11 236307 13 236819 14 237331 ...</td>\n","      <td>(273, 459, 24, 22)</td>\n","      <td>384</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17516</th>\n","      <td>17516</td>\n","      <td>ffd3d193c71e</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>138217 4 138728 6 139239 8 139751 8 140262 10 ...</td>\n","      <td>(476, 269, 25, 120)</td>\n","      <td>1789</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17517</th>\n","      <td>17517</td>\n","      <td>ffd3d193c71e</td>\n","      <td>1</td>\n","      <td>blood_vessel</td>\n","      <td>57 8 568 10 1080 12 1591 14 2103 14 2615 15 31...</td>\n","      <td>(54, 0, 15, 16)</td>\n","      <td>186</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17518 rows × 8 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0      image_id  category_id category_name  \\\n","0               0  0006ff2aa7cd            2    glomerulus   \n","1               1  0006ff2aa7cd            1  blood_vessel   \n","2               2  0006ff2aa7cd            1  blood_vessel   \n","3               3  0006ff2aa7cd            1  blood_vessel   \n","4               4  0006ff2aa7cd            1  blood_vessel   \n","...           ...           ...          ...           ...   \n","17513       17513  ffd3d193c71e            1  blood_vessel   \n","17514       17514  ffd3d193c71e            1  blood_vessel   \n","17515       17515  ffd3d193c71e            1  blood_vessel   \n","17516       17516  ffd3d193c71e            1  blood_vessel   \n","17517       17517  ffd3d193c71e            1  blood_vessel   \n","\n","                                             annotations                 bbox  \\\n","0      2 253 513 254 1025 254 1537 254 2049 254 2561 ...     (0, 0, 271, 250)   \n","1      37657 3 38167 7 38678 8 39190 10 39701 11 4021...    (272, 73, 20, 37)   \n","2      121934 3 122444 7 122955 9 123466 10 123978 11...    (72, 238, 37, 55)   \n","3      199677 4 200187 6 200697 8 201208 9 201719 10 ...   (489, 389, 23, 54)   \n","4      222553 10 223053 5 223063 13 223564 25 224073 ...   (326, 434, 51, 44)   \n","...                                                  ...                  ...   \n","17513  146108 3 146618 6 147128 8 147639 12 148149 15...   (173, 285, 23, 24)   \n","17514  16940 3 17451 5 17962 7 18473 8 18984 9 19495 ...     (28, 33, 21, 60)   \n","17515  235285 8 235796 11 236307 13 236819 14 237331 ...   (273, 459, 24, 22)   \n","17516  138217 4 138728 6 139239 8 139751 8 140262 10 ...  (476, 269, 25, 120)   \n","17517  57 8 568 10 1080 12 1591 14 2103 14 2615 15 31...      (54, 0, 15, 16)   \n","\n","        area  iscrowd  \n","0      56426        0  \n","1        570        0  \n","2       1027        0  \n","3        996        0  \n","4       1055        0  \n","...      ...      ...  \n","17513    396        0  \n","17514    736        0  \n","17515    384        0  \n","17516   1789        0  \n","17517    186        0  \n","\n","[17518 rows x 8 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df_"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Dataset & Dataloader"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["ds_train = HuBMAPDataset(TRAIN_PATH, df_train, resize=resize_factor, transforms=get_transform(train=True))\n","dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n","                      num_workers=0, collate_fn=lambda x: tuple(zip(*x)))\n","\n","ds_val = HuBMAPDataset(TRAIN_PATH, df_val, resize=resize_factor, transforms=get_transform(train=False))\n","dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n","                    num_workers=0, collate_fn=lambda x: tuple(zip(*x)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Set-up Model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["cache_dir = os.path.expanduser(\"~/.cache/torch/hub/checkpoints/\")\n","os.makedirs(cache_dir, exist_ok=True)\n","!cp /Users/admin/Documents/GitHub/HuBMAP-Hacking_the_Human_Vasculature/DATA/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth $cache_dir\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def get_model(num_classes, model_chkpt=None):    \n","    if NORMALIZE:\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n","                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n","                                                                   image_mean=RESNET_MEAN,\n","                                                                   image_std=RESNET_STD)\n","    else:\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n","                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG)\n","\n","    # get the number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n","    if model_chkpt:\n","        model.load_state_dict(torch.load(model_chkpt, map_location=DEVICE))\n","    return model\n","# Get the Mask R-CNN model\n","# The model does classification, bounding boxes and MASKs for individuals, all at the same time\n","# We only care about MASKS\n","model = get_model(len(cell_type_dict))\n","model.to(DEVICE)\n","\n","# TODO: try removing this for\n","for param in model.parameters():\n","    param.requires_grad = True\n","    \n","model.train();"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["DataParallel(\n","  (module): MaskRCNN(\n","    (transform): GeneralizedRCNNTransform(\n","        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","    )\n","    (backbone): BackboneWithFPN(\n","      (body): IntermediateLayerGetter(\n","        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","        (relu): ReLU(inplace=True)\n","        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","        (layer1): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","              (1): FrozenBatchNorm2d(256, eps=0.0)\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","        (layer2): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): FrozenBatchNorm2d(512, eps=0.0)\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (3): Bottleneck(\n","            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","        (layer3): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): FrozenBatchNorm2d(1024, eps=0.0)\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (3): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (4): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (5): Bottleneck(\n","            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","        (layer4): Sequential(\n","          (0): Bottleneck(\n","            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","            (downsample): Sequential(\n","              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","              (1): FrozenBatchNorm2d(2048, eps=0.0)\n","            )\n","          )\n","          (1): Bottleneck(\n","            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","          (2): Bottleneck(\n","            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","            (relu): ReLU(inplace=True)\n","          )\n","        )\n","      )\n","      (fpn): FeaturePyramidNetwork(\n","        (inner_blocks): ModuleList(\n","          (0): Conv2dNormActivation(\n","            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (1): Conv2dNormActivation(\n","            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (2): Conv2dNormActivation(\n","            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (3): Conv2dNormActivation(\n","            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","        (layer_blocks): ModuleList(\n","          (0-3): 4 x Conv2dNormActivation(\n","            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          )\n","        )\n","        (extra_blocks): LastLevelMaxPool()\n","      )\n","    )\n","    (rpn): RegionProposalNetwork(\n","      (anchor_generator): AnchorGenerator()\n","      (head): RPNHead(\n","        (conv): Sequential(\n","          (0): Conv2dNormActivation(\n","            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (1): ReLU(inplace=True)\n","          )\n","        )\n","        (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","        (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","    )\n","    (roi_heads): RoIHeads(\n","      (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","      (box_head): TwoMLPHead(\n","        (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","        (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","      )\n","      (box_predictor): FastRCNNPredictor(\n","        (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n","        (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n","      )\n","      (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n","      (mask_head): MaskRCNNHeads(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (mask_predictor): MaskRCNNPredictor(\n","        (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","        (relu): ReLU(inplace=True)\n","        (mask_fcn_logits): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","    )\n","  )\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from torch.nn.parallel import DataParallel\n","# DataParallel로 모델 감싸기\n","model = DataParallel(model)\n","\n","# 장치 설정\n","\n","model.to(DEVICE)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["!export PYTORCH_ENABLE_MPS_FALLBACK=1\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting epoch 1 of 1\n"]}],"source":["params = [p for p in model.parameters() if p.requires_grad]\n","# optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","n_batches, n_batches_val = len(dl_train), len(dl_val)\n","validation_mask_losses = []\n","\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n","\n","    time_start = time.time()\n","    loss_accum = 0.0\n","    loss_mask_accum = 0.0\n","    loss_classifier_accum = 0.0\n","    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n","    \n","        # Predict\n","        images = list(image.to(DEVICE) for image in images)\n","        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        loss = sum(loss for loss in loss_dict.values())\n","        \n","        # Backprop\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Logging\n","        loss_mask = loss_dict['loss_mask'].item()\n","        loss_accum += loss.item()\n","        loss_mask_accum += loss_mask\n","        loss_classifier_accum += loss_dict['loss_classifier'].item()\n","        \n","        if batch_idx % 500 == 0:\n","            print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}.\")\n","                        \n","    if USE_SCHEDULER:\n","        lr_scheduler.step()\n","\n","    # Train losses\n","    train_loss = loss_accum / n_batches\n","    train_loss_mask = loss_mask_accum / n_batches\n","    train_loss_classifier = loss_classifier_accum / n_batches\n","\n","    # Validation\n","    val_loss_accum = 0\n","    val_loss_mask_accum = 0\n","    val_loss_classifier_accum = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n","            images = list(image.to(DEVICE) for image in images)\n","            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","\n","            val_loss_dict = model(images, targets)\n","            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n","            val_loss_accum += val_batch_loss.item()\n","            val_loss_mask_accum += val_loss_dict['loss_mask'].item()\n","            val_loss_classifier_accum += val_loss_dict['loss_classifier'].item()\n","\n","    # Validation losses\n","    val_loss = val_loss_accum / n_batches_val\n","    val_loss_mask = val_loss_mask_accum / n_batches_val\n","    val_loss_classifier = val_loss_classifier_accum / n_batches_val\n","    elapsed = time.time() - time_start\n","\n","    validation_mask_losses.append(val_loss_mask)\n","\n","    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n","    prefix = f\"[Epoch {epoch:2d}/{NUM_EPOCHS:2d}]\"\n","    print(f\"{prefix} -- train mask loss: {train_loss_mask:7.3f}, classes loss {train_loss_classifier:7.3f}\")\n","    print(f\"{prefix} -- val mask loss  : {val_loss_mask:7.3f}, classes loss {val_loss_classifier:7.3f}\")\n","    print(f\"{prefix} -- train loss: {train_loss:7.3f}. val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n","    print(\"---------------------------------------------------------------------------------------------\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              image          mask                            path\n","0  0b7772cced70.tif  0b7772cced70  ../DATA/train/0b7772cced70.tif\n","1  a212a174533b.tif  a212a174533b  ../DATA/train/a212a174533b.tif\n","2  bb14110c0ad9.tif  bb14110c0ad9  ../DATA/train/bb14110c0ad9.tif\n","3  9877c0dfede4.tif  9877c0dfede4  ../DATA/train/9877c0dfede4.tif\n","4  9227f6ede08f.tif  9227f6ede08f  ../DATA/train/9227f6ede08f.tif\n","(7033, 3)\n"]}],"source":["all_files_list = os.listdir(TRAIN_PATH)\n","\n","# 이미지 파일명과 마스크 파일명을 저장할 리스트 생성\n","image_file_list = []\n","mask_file_list = []\n","image_file_path_list = []\n","# 모든 파일에 대해서 반복\n","for file in all_files_list:\n","    image_files_path = os.path.join(TRAIN_PATH, file)\n","\n","    # 이미지 파일인 경우\n","    if file.endswith('.tif'):\n","        # 이미지 파일명을 리스트에 추가\n","        image_file_list.append(file)\n","        \n","        # 마스크 파일명 생성\n","        mask_file = file.replace('.tif', '')\n","        \n","        # 마스크 파일명을 리스트에 추가\n","        mask_file_list.append(mask_file)\n","\n","        # 이미지 파일 경로를 리스트에 추가\n","        image_file_path_list.append(image_files_path)\n","\n","# 데이터 프레임 생성\n","df = pd.DataFrame({'image': image_file_list, 'mask': mask_file_list, 'path': image_file_path_list})\n","\n","# 데이터 프레임 출력\n","print(df.head())\n","print(df.shape)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>mask</th>\n","      <th>path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0b7772cced70.tif</td>\n","      <td>0b7772cced70</td>\n","      <td>../DATA/train/0b7772cced70.tif</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>a212a174533b.tif</td>\n","      <td>a212a174533b</td>\n","      <td>../DATA/train/a212a174533b.tif</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bb14110c0ad9.tif</td>\n","      <td>bb14110c0ad9</td>\n","      <td>../DATA/train/bb14110c0ad9.tif</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9877c0dfede4.tif</td>\n","      <td>9877c0dfede4</td>\n","      <td>../DATA/train/9877c0dfede4.tif</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9227f6ede08f.tif</td>\n","      <td>9227f6ede08f</td>\n","      <td>../DATA/train/9227f6ede08f.tif</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7028</th>\n","      <td>7f34441a1f79.tif</td>\n","      <td>7f34441a1f79</td>\n","      <td>../DATA/train/7f34441a1f79.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7029</th>\n","      <td>3f23a41f4bc4.tif</td>\n","      <td>3f23a41f4bc4</td>\n","      <td>../DATA/train/3f23a41f4bc4.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7030</th>\n","      <td>d20e93312427.tif</td>\n","      <td>d20e93312427</td>\n","      <td>../DATA/train/d20e93312427.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7031</th>\n","      <td>f2683290d169.tif</td>\n","      <td>f2683290d169</td>\n","      <td>../DATA/train/f2683290d169.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7032</th>\n","      <td>07edb2d7b7db.tif</td>\n","      <td>07edb2d7b7db</td>\n","      <td>../DATA/train/07edb2d7b7db.tif</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7033 rows × 3 columns</p>\n","</div>"],"text/plain":["                 image          mask                            path\n","0     0b7772cced70.tif  0b7772cced70  ../DATA/train/0b7772cced70.tif\n","1     a212a174533b.tif  a212a174533b  ../DATA/train/a212a174533b.tif\n","2     bb14110c0ad9.tif  bb14110c0ad9  ../DATA/train/bb14110c0ad9.tif\n","3     9877c0dfede4.tif  9877c0dfede4  ../DATA/train/9877c0dfede4.tif\n","4     9227f6ede08f.tif  9227f6ede08f  ../DATA/train/9227f6ede08f.tif\n","...                ...           ...                             ...\n","7028  7f34441a1f79.tif  7f34441a1f79  ../DATA/train/7f34441a1f79.tif\n","7029  3f23a41f4bc4.tif  3f23a41f4bc4  ../DATA/train/3f23a41f4bc4.tif\n","7030  d20e93312427.tif  d20e93312427  ../DATA/train/d20e93312427.tif\n","7031  f2683290d169.tif  f2683290d169  ../DATA/train/f2683290d169.tif\n","7032  07edb2d7b7db.tif  07edb2d7b7db  ../DATA/train/07edb2d7b7db.tif\n","\n","[7033 rows x 3 columns]"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>mask</th>\n","      <th>path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0b7772cced70.tif</td>\n","      <td>0b7772cced70</td>\n","      <td>../DATA/train/0b7772cced70.tif</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>a212a174533b.tif</td>\n","      <td>a212a174533b</td>\n","      <td>../DATA/train/a212a174533b.tif</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bb14110c0ad9.tif</td>\n","      <td>bb14110c0ad9</td>\n","      <td>../DATA/train/bb14110c0ad9.tif</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9877c0dfede4.tif</td>\n","      <td>9877c0dfede4</td>\n","      <td>../DATA/train/9877c0dfede4.tif</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9227f6ede08f.tif</td>\n","      <td>9227f6ede08f</td>\n","      <td>../DATA/train/9227f6ede08f.tif</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7028</th>\n","      <td>7f34441a1f79.tif</td>\n","      <td>7f34441a1f79</td>\n","      <td>../DATA/train/7f34441a1f79.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7029</th>\n","      <td>3f23a41f4bc4.tif</td>\n","      <td>3f23a41f4bc4</td>\n","      <td>../DATA/train/3f23a41f4bc4.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7030</th>\n","      <td>d20e93312427.tif</td>\n","      <td>d20e93312427</td>\n","      <td>../DATA/train/d20e93312427.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7031</th>\n","      <td>f2683290d169.tif</td>\n","      <td>f2683290d169</td>\n","      <td>../DATA/train/f2683290d169.tif</td>\n","    </tr>\n","    <tr>\n","      <th>7032</th>\n","      <td>07edb2d7b7db.tif</td>\n","      <td>07edb2d7b7db</td>\n","      <td>../DATA/train/07edb2d7b7db.tif</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7033 rows × 3 columns</p>\n","</div>"],"text/plain":["                 image          mask                            path\n","0     0b7772cced70.tif  0b7772cced70  ../DATA/train/0b7772cced70.tif\n","1     a212a174533b.tif  a212a174533b  ../DATA/train/a212a174533b.tif\n","2     bb14110c0ad9.tif  bb14110c0ad9  ../DATA/train/bb14110c0ad9.tif\n","3     9877c0dfede4.tif  9877c0dfede4  ../DATA/train/9877c0dfede4.tif\n","4     9227f6ede08f.tif  9227f6ede08f  ../DATA/train/9227f6ede08f.tif\n","...                ...           ...                             ...\n","7028  7f34441a1f79.tif  7f34441a1f79  ../DATA/train/7f34441a1f79.tif\n","7029  3f23a41f4bc4.tif  3f23a41f4bc4  ../DATA/train/3f23a41f4bc4.tif\n","7030  d20e93312427.tif  d20e93312427  ../DATA/train/d20e93312427.tif\n","7031  f2683290d169.tif  f2683290d169  ../DATA/train/f2683290d169.tif\n","7032  07edb2d7b7db.tif  07edb2d7b7db  ../DATA/train/07edb2d7b7db.tif\n","\n","[7033 rows x 3 columns]"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["class ASPP(nn.Module):\n","    def __init__(self, in_channels, out_channels, dilations):\n","        super(ASPP, self).__init__()\n","        self.aspp_blocks = nn.ModuleList([\n","            nn.Conv2d(in_channels, out_channels, 1),\n","            nn.Conv2d(in_channels, out_channels, 3, padding=dilations[0], dilation=dilations[0]),\n","            nn.Conv2d(in_channels, out_channels, 3, padding=dilations[1], dilation=dilations[1]),\n","            nn.Conv2d(in_channels, out_channels, 3, padding=dilations[2], dilation=dilations[2])\n","        ])\n","        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                             nn.Conv2d(in_channels, out_channels, 1))        \n","    def forward(self, x):\n","        x_aspp = torch.cat([aspp(x) for aspp in self.aspp_blocks], dim=1)\n","        x_gap = self.global_avg_pool(x)\n","        x_gap = F.interpolate(x_gap, size=x_aspp.size()[2:], mode='bilinear', align_corners=True)\n","        return torch.cat([x_aspp, x_gap], dim=1)\n","\n","\n","class DeepLabV3Plus(nn.Module):\n","    def __init__(self, num_classes=4, dilations=[1, 6, 12]):\n","        super(DeepLabV3Plus, self).__init__()\n","        # Use resnet101 in torchvision as backbone\n","        resnet101 = models.resnet101(pretrained=True)\n","        self.backbone = nn.Sequential(*list(resnet101.children())[:-2])\n","\n","        self.aspp = ASPP(2048, 256, dilations)  # Atrous Spatial Pyramid Pooling module\n","        self.conv1 = nn.Conv2d(256, 48, 1, bias=False)  # Low-level features\n","        self.bn1 = nn.BatchNorm2d(48)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","        \n","        self.last_conv = nn.Sequential(\n","            nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),  # Changed input channels from 304 to 1072\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n","        )\n","        \n","    def forward(self, x):\n","        input_size = x.size()[2:]\n","        low_level_feat = self.backbone[0:5](x)\n","        output_feature = self.backbone[5:](low_level_feat)\n","\n","        output_feature = self.aspp(output_feature)\n","        output_feature = F.interpolate(output_feature, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)\n","\n","        low_level_feat = self.conv1(low_level_feat)\n","        low_level_feat = self.bn1(low_level_feat)\n","        low_level_feat = self.relu(low_level_feat)\n","        low_level_feat = self.dropout(low_level_feat)\n","\n","        x = torch.cat((output_feature, low_level_feat), dim=1)\n","        x = self.last_conv(x)\n","        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n","\n","        return x\n","\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["class DeepLabV3Plus(nn.Module):\n","    def __init__(self, num_classes=4, dilations=[1, 6, 12], pretrained=True):\n","        super(DeepLabV3Plus, self).__init__()\n","        # Use resnet101 in torchvision as backbone\n","        resnet101 = models.resnet101(pretrained=pretrained)\n","        self.backbone = nn.Sequential(*list(resnet101.children())[:-2])\n","\n","        # Freeze the backbone layers\n","        if pretrained:\n","            for param in self.backbone.parameters():\n","                param.requires_grad = False\n","\n","        self.aspp = ASPP(2048, 256, dilations)  # Atrous Spatial Pyramid Pooling module\n","        self.conv1 = nn.Conv2d(256, 48, 1, bias=False)  # Low-level features\n","        self.bn1 = nn.BatchNorm2d(48)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","        \n","        self.last_conv = nn.Sequential(\n","            nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n","        )\n","        \n","    def forward(self, x):\n","        input_size = x.size()[2:]\n","        low_level_feat = self.backbone[0:5](x)\n","        output_feature = self.backbone[5:](low_level_feat)\n","\n","        output_feature = self.aspp(output_feature)\n","        output_feature = F.interpolate(output_feature, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)\n","\n","        low_level_feat = self.conv1(low_level_feat)\n","        low_level_feat = self.bn1(low_level_feat)\n","        low_level_feat = self.relu(low_level_feat)\n","        low_level_feat = self.dropout(low_level_feat)\n","\n","        x = torch.cat((output_feature, low_level_feat), dim=1)\n","        x = self.last_conv(x)\n","        x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["from pycocotools import mask as mask_utils\n","\n","class MAPIOUEvaluator:\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","        self.reset()\n","\n","    def reset(self):\n","        self.scores = []\n","\n","    def process(self, inputs, outputs):\n","        for inp, out in zip(inputs, outputs):\n","            if out is None:\n","                self.scores.append(0)\n","            else:\n","                image_id = inp['image_id']\n","                pred_mask = out.squeeze().detach().cpu().numpy() > 0.5  # apply threshold\n","                enc_pred = mask_utils.encode(np.asfortranarray(pred_mask.astype(\"uint8\")))\n","\n","                # Get the ground truth mask\n","                gt_mask = inp['mask']\n","                enc_gt = mask_utils.encode(np.asfortranarray(gt_mask.astype(\"uint8\")))\n","\n","                # Compute the IoU\n","                iou = mask_utils.iou([enc_pred], [enc_gt], [0])[0, 0]\n","                self.scores.append(iou)\n","\n","    def evaluate(self):\n","        return {\"MaP IoU\": np.mean(self.scores)}\n"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","train_transform = A.Compose([\n","                            A.Resize(64, 64),\n","                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n","                            ToTensorV2()\n","                            ])\n","\n","test_transform = A.Compose([\n","                            A.Resize(64, 64),\n","                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n","                            ToTensorV2()\n","                            ])\n","class PolygonDataset(Dataset):\n","    def __init__(self, annotations_df, json_path, transform=None):\n","        super(PolygonDataset, self).__init__()\n","        \n","        self.img_files = annotations_df['image']\n","        self.img_paths = annotations_df['path']\n","        self.transform = transform\n","\n","        # Load all data from the json file\n","        self.annotations = {}\n","        with open(json_path, 'r') as file:\n","            for line in file:\n","                data = json.loads(line)\n","                # Match image id and its annotations\n","                self.annotations[data['id']] = data['annotations']\n","\n","    def __len__(self):\n","        return len(self.img_files)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = self.img_paths[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        img = img.resize((128, 128))  \n","\n","        # Image id is the filename without extension\n","        img_id = os.path.splitext(self.img_files[idx])[0]\n","\n","        # Find the corresponding annotations using image id\n","        annotations = self.annotations.get(img_id, [])\n","\n","        # Create mask\n","        mask = np.zeros((img.size[1], img.size[0]), dtype=np.uint8)\n","        for annotation in annotations:\n","            for polygon in annotation['coordinates']:\n","                contour = np.array(polygon, dtype=np.int32)\n","                cv2.fillPoly(mask, [contour], color=(255))\n","\n","        # If a transform function is given, apply it on the image and the mask\n","        if self.transform:\n","            transformed = self.transform(image=np.array(img), mask=mask)\n","            img = transformed['image']\n","            mask = transformed['mask']\n","\n","        return {'image': img, 'mask': mask, 'image_id': img_id}\n","\n","\n","# define parameters\n","batch_size = 16\n","learning_rate = 0.001\n","num_epochs = 50\n","\n","# 데이터를 훈련 세트와 검증 세트로 분할\n","train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# 인덱스 초기화\n","train_data = train_data.reset_index(drop=True)\n","val_data = val_data.reset_index(drop=True)\n","\n","# create datasets\n","\n","# Create datasets\n","train_dataset = PolygonDataset(train_data, json_path=os.path.join(BASE_PATH, 'polygons.jsonl'), transform=train_transform)\n","val_dataset = PolygonDataset(val_data, json_path=os.path.join(BASE_PATH, 'polygons.jsonl'), transform=test_transform)\n","\n","# create dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# create model\n","model = DeepLabV3Plus(num_classes=4, dilations=[1, 6, 12, 18]).to(DEVICE)  # initialize model\n","\n","# define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()  # loss function\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # optimizer\n"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_loader))[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mshape, \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader))[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape\n","\u001b[0;31mKeyError\u001b[0m: 0"]}],"source":["next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Given groups=1, weight of size [256, 304, 3, 3], expected input[16, 1328, 16, 16] to have 304 channels, but got 1328 channels instead","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     20\u001b[0m \u001b[39m# backward and optimize\u001b[39;00m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[48], line 58\u001b[0m, in \u001b[0;36mDeepLabV3Plus.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m low_level_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(low_level_feat)\n\u001b[1;32m     57\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((output_feature, low_level_feat), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_conv(x)\n\u001b[1;32m     59\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(x, size\u001b[39m=\u001b[39minput_size, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 304, 3, 3], expected input[16, 1328, 16, 16] to have 304 channels, but got 1328 channels instead"]}],"source":["def dice_score(output, target, eps=1e-7):\n","    output = output.view(-1).detach().cpu().numpy()\n","    target = target.view(-1).detach().cpu().numpy()\n","    intersection = (output * target).sum()\n","    return (2. * intersection) / (output.sum() + target.sum() + eps)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    running_dice_score = 0.0\n","\n","    for images, masks in train_loader:\n","        images = images.to(device)\n","        masks = masks.to(device)\n","\n","        # forward\n","        outputs = model(images)\n","        loss = criterion(outputs, masks)\n","\n","        # backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        running_dice_score += dice_score(outputs, masks) * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_dice_score = running_dice_score / len(train_loader)\n","\n","    print('Train Epoch: {}, Loss: {:.4f}, Dice Score: {:.4f}'.format(epoch, epoch_loss, epoch_dice_score))\n","\n","    # validate the model\n","    model.eval()\n","    running_loss = 0.0\n","    running_dice_score = 0.0\n","\n","    with torch.no_grad():\n","        for images, masks in val_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            running_loss += loss.item() * images.size(0)\n","            running_dice_score += dice_score(outputs, masks) * images.size(0)\n","\n","    epoch_loss = running_loss / len(val_loader)\n","    epoch_dice_score = running_dice_score / len(val_loader)\n","\n","    print('Validation Epoch: {}, Loss: {:.4f}, Dice Score: {:.4f}'.format(epoch, epoch_loss, epoch_dice_score))\n","\n","    # print statistics or save the model\n","    # your code here\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = pd.read_csv(TEST_CSV)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataset = CustomDataset(test['img_path'].values, None, test_transform)\n","test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def inference(model, test_loader, device):\n","    model.eval()\n","    preds = []\n","    with torch.no_grad():\n","        for imgs in tqdm(iter(test_loader)):\n","            imgs = imgs.float().to(device)\n","            \n","            pred = model(imgs)\n","            \n","            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n","    \n","    preds = le.inverse_transform(preds)\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee96428cd9094bdb9c009af9a02a5d62","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["preds = inference(infer_model, test_loader, device)"]},{"cell_type":"markdown","metadata":{},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submit = pd.read_csv(SAMPLE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submit['label'] = preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datetime import datetime, timedelta, timezone\n","# 시간 고유값 \n","kst = timezone(timedelta(hours=9))        \n","train_serial = datetime.now(tz=kst).strftime(\"%Y%m%d_%H%M%S\")\n","\n","# 기록 경로\n","RECORDER_DIR = os.path.join(DATA_PATH, 'results', train_serial)\n","\n","# 현재 시간 기준 폴더 생성\n","os.makedirs(RECORDER_DIR, exist_ok=True)    \n","RESULT_PATH = os.path.join(RECORDER_DIR, 'submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submit.to_csv(RESULT_PATH, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
